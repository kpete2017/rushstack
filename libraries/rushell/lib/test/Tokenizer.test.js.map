{"version":3,"file":"Tokenizer.test.js","sourceRoot":"","sources":["../../src/test/Tokenizer.test.ts"],"names":[],"mappings":";AAAA,4FAA4F;AAC5F,2DAA2D;;AAE3D,4CAA2D;AAE3D,SAAS,MAAM,CAAC,CAAS;IACvB,OAAO,CAAC,CAAC,OAAO,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC,OAAO,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC,OAAO,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC,OAAO,CAAC,KAAK,EAAE,KAAK,CAAC,CAAC;AACnG,CAAC;AAED,SAAS,QAAQ,CAAC,KAAa;IAC7B,MAAM,SAAS,GAAc,IAAI,qBAAS,CAAC,KAAK,CAAC,CAAC;IAClD,OAAO,SAAS,CAAC,UAAU,EAAE,CAAC;AAChC,CAAC;AAED,SAAS,aAAa,CAAC,KAAa;IAClC,MAAM,SAAS,GAAc,IAAI,qBAAS,CAAC,KAAK,CAAC,CAAC;IAElD,MAAM,cAAc,GAAsC,SAAS,CAAC,UAAU,EAAE,CAAC,GAAG,CAAC,CAAC,KAAK,EAAE,EAAE;QAC7F,OAAO;YACL,IAAI,EAAE,qBAAS,CAAC,KAAK,CAAC,IAAI,CAAC;YAC3B,KAAK,EAAE,MAAM,CAAC,KAAK,CAAC,QAAQ,EAAE,CAAC;SAChC,CAAC;IACJ,CAAC,CAAC,CAAC;IAEH,MAAM,CAAC;QACL,KAAK,EAAE,MAAM,CAAC,SAAS,CAAC,KAAK,CAAC,QAAQ,EAAE,CAAC;QACzC,MAAM,EAAE,cAAc;KACvB,CAAC,CAAC,eAAe,EAAE,CAAC;AACvB,CAAC;AAED,IAAI,CAAC,kBAAkB,EAAE,GAAG,EAAE;IAC5B,aAAa,CAAC,EAAE,CAAC,CAAC;IAClB,aAAa,CAAC,MAAM,CAAC,CAAC;AACxB,CAAC,CAAC,CAAC;AAEH,IAAI,CAAC,wBAAwB,EAAE,GAAG,EAAE;IAClC,aAAa,CAAC,kCAAkC,CAAC,CAAC;AACpD,CAAC,CAAC,CAAC;AAEH,IAAI,CAAC,uBAAuB,EAAE,GAAG,EAAE;IACjC,aAAa,CAAC,uBAAuB,CAAC,CAAC;IACvC,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,kBAAkB,CAAC,CAAC,CAAC,YAAY,EAAE,CAAC;AAC5D,CAAC,CAAC,CAAC;AAEH,IAAI,CAAC,qBAAqB,EAAE,GAAG,EAAE;IAC/B,aAAa,CAAC,cAAc,CAAC,CAAC;IAC9B,aAAa,CAAC,KAAK,CAAC,CAAC;IACrB,aAAa,CAAC,IAAI,CAAC,CAAC;IACpB,aAAa,CAAC,GAAG,CAAC,CAAC;AACrB,CAAC,CAAC,CAAC;AAEH,IAAI,CAAC,sBAAsB,EAAE,GAAG,EAAE;IAChC,aAAa,CAAC,aAAa,CAAC,CAAC;IAC7B,aAAa,CAAC,SAAS,CAAC,CAAC;IACzB,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,GAAG,CAAC,CAAC,CAAC,YAAY,EAAE,CAAC;IAC3C,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,QAAQ,CAAC,CAAC,CAAC,YAAY,EAAE,CAAC;AAClD,CAAC,CAAC,CAAC;AAEH,IAAI,CAAC,2BAA2B,EAAE,GAAG,EAAE;IACrC,aAAa,CAAC,cAAc,CAAC,CAAC;IAC9B,aAAa,CAAC,YAAY,CAAC,CAAC;IAC5B,aAAa,CAAC,eAAe,CAAC,CAAC;IAC/B,aAAa,CAAC,iCAAiC,CAAC,CAAC;IACjD,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,iBAAiB,CAAC,CAAC,CAAC,YAAY,EAAE,CAAC;IACzD,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,oBAAoB,CAAC,CAAC,CAAC,YAAY,EAAE,CAAC;IAC5D,MAAM,CAAC,GAAG,EAAE,CAAC,QAAQ,CAAC,sBAAsB,CAAC,CAAC,CAAC,YAAY,EAAE,CAAC;AAChE,CAAC,CAAC,CAAC","sourcesContent":["// Copyright (c) Microsoft Corporation. All rights reserved. Licensed under the MIT license.\r\n// See LICENSE in the project root for license information.\r\n\r\nimport { Tokenizer, TokenKind, Token } from '../Tokenizer';\r\n\r\nfunction escape(s: string): string {\r\n  return s.replace(/\\n/g, '[n]').replace(/\\r/g, '[r]').replace(/\\t/g, '[t]').replace(/\\\\/g, '[b]');\r\n}\r\n\r\nfunction tokenize(input: string): Token[] {\r\n  const tokenizer: Tokenizer = new Tokenizer(input);\r\n  return tokenizer.readTokens();\r\n}\r\n\r\nfunction matchSnapshot(input: string): void {\r\n  const tokenizer: Tokenizer = new Tokenizer(input);\r\n\r\n  const reportedTokens: { kind: string; value: string }[] = tokenizer.readTokens().map((token) => {\r\n    return {\r\n      kind: TokenKind[token.kind],\r\n      value: escape(token.toString())\r\n    };\r\n  });\r\n\r\n  expect({\r\n    input: escape(tokenizer.input.toString()),\r\n    tokens: reportedTokens\r\n  }).toMatchSnapshot();\r\n}\r\n\r\ntest('00: empty inputs', () => {\r\n  matchSnapshot('');\r\n  matchSnapshot('\\r\\n');\r\n});\r\n\r\ntest('01: white space tokens', () => {\r\n  matchSnapshot(' \\t abc   \\r\\ndef  \\n  ghi\\n\\r  ');\r\n});\r\n\r\ntest('02: text with escapes', () => {\r\n  matchSnapshot(' ab+56\\\\>qrst(abc\\\\))');\r\n  expect(() => tokenize('Unterminated: \\\\')).toThrowError();\r\n});\r\n\r\ntest('03: The && operator', () => {\r\n  matchSnapshot('&&abc&&cde&&');\r\n  matchSnapshot('a&b');\r\n  matchSnapshot('&&');\r\n  matchSnapshot('&');\r\n});\r\n\r\ntest('04: dollar variables', () => {\r\n  matchSnapshot('$abc123.456');\r\n  matchSnapshot('$ab$_90');\r\n  expect(() => tokenize('$')).toThrowError();\r\n  expect(() => tokenize('${abc}')).toThrowError();\r\n});\r\n\r\ntest('05: double-quoted strings', () => {\r\n  matchSnapshot('what \"is\" is');\r\n  matchSnapshot('what\"is\"is');\r\n  matchSnapshot('what\"is\\\\\"\"is');\r\n  matchSnapshot('no C-style escapes: \"\\\\t\\\\r\\\\n\"');\r\n  expect(() => tokenize('Unterminated: \"')).toThrowError();\r\n  expect(() => tokenize('Unterminated: \"abc')).toThrowError();\r\n  expect(() => tokenize('Unterminated: \"abc\\\\')).toThrowError();\r\n});\r\n"]}